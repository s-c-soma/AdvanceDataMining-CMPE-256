{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMPE256_Assignment_9_ML-BasedSpamFilter.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM7SCrN2y2IVwBXDkJm15rR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-c-soma/AdvanceDataMining-CMPE-256/blob/main/assignments/CMPE256_Assignment_9_ML_BasedSpamFilter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCqv5rNRvQd7"
      },
      "source": [
        "# ML Based Spam Filter\n",
        "\n",
        "**  Steps for implementing Spam Filter: **\n",
        "I have used multiple techniques to implement spam filter.\n",
        "\n",
        "1. From the hand computation, I have computed Tf/IDF vector for each documents as well as Spam dictionary\n",
        "2. Here I am implementing the similarity computation between a document and the spam dictionary\n",
        "3. Then we can decide a threshold value\n",
        "4. If the similarity score crosses that threshold value, we can declare the document as spam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mw76TJq3hJy"
      },
      "source": [
        "# Method_1: Using Cosine Similarity from Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyNunCP94mca"
      },
      "source": [
        "from scipy.spatial import distance"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeYG7VqE4s8E"
      },
      "source": [
        "VD1 = [0.2, 0, 0.43, 0, 0, 0, 0, 0, 0.3, 0.43, 0, 0, 0, 0]\n",
        "VD2 = [0.2, 0, 0.43, 0, 0, 0, 0, 0, 0.3, 0.43, 0, 0, 0, 0]\n",
        "VD3 = [0.2, 0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0.666, 0, 0, 0]\n",
        "VD4 = [0.222, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.43, 0]\n",
        "VD5 = [0, 0, 0, 0, 0, 0.6, 0, 0.477, 0.333, 0, 0, 0.6, 0.43, 0]\n",
        "VD6 = [0, 0, 0, 0, 0, 0, 0, 0.516, 0, 0, 0, 0, 0, 0.6]\n",
        "VSPD = [0.2, 0.6, 0.43, 0.9, 0.9, 0.6, 0.6, 0.43, 0.3, 0.43, 0.6, 0.6, 0.43, 0.6]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2R0Nnj13p5Q",
        "outputId": "474bdcf4-0f7e-49b0-90e1-fb28edb4d34e"
      },
      "source": [
        "doc1_cosine_similarity = 1 - distance.cosine(VD1, VSPD)\n",
        "print(\"doc1_cosine_similarity=\", doc1_cosine_similarity)\n",
        "\n",
        "doc2_cosine_similarity = 1 - distance.cosine(VD2, VSPD)\n",
        "print(\"doc2_cosine_similarity=\", doc2_cosine_similarity)\n",
        "\n",
        "doc3_cosine_similarity = 1 - distance.cosine(VD3, VSPD)\n",
        "print(\"doc3_cosine_similarity=\", doc3_cosine_similarity)\n",
        "\n",
        "doc4_cosine_similarity = 1 - distance.cosine(VD4, VSPD)\n",
        "print(\"doc4_cosine_similarity=\", doc4_cosine_similarity)\n",
        "\n",
        "doc5_cosine_similarity = 1 - distance.cosine(VD5, VSPD)\n",
        "print(\"doc5_cosine_similarity=\", doc5_cosine_similarity)\n",
        "\n",
        "doc6_cosine_similarity = 1 - distance.cosine(VD6, VSPD)\n",
        "print(\"doc6_cosine_similarity=\", doc6_cosine_similarity)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc1_cosine_similarity= 0.32786143061900863\n",
            "doc2_cosine_similarity= 0.32786143061900863\n",
            "doc3_cosine_similarity= 0.40374581130590104\n",
            "doc4_cosine_similarity= 0.354542390210272\n",
            "doc5_cosine_similarity= 0.5032156733340074\n",
            "doc6_cosine_similarity= 0.3409963135780971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLtjI27owqL1"
      },
      "source": [
        "# Method_2: Using Library- Direct from Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgDNQ5yXuu7q",
        "outputId": "26481393-8555-49fe-fcda-88c3d26b86c0"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_8e5vVKw0X7"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi31CoX0xmxx"
      },
      "source": [
        "## Spam Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3anSL7Hkw4EB"
      },
      "source": [
        "spam_vocab = nlp(u'Free Click here  visit open attachment call this number money Out extra offer  available Pension Opportunity Chance Investment Pension')\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk0IvtNJxqeS"
      },
      "source": [
        "## With documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YzfxKVsxi8z",
        "outputId": "ea8ebf24-8045-4546-98a3-f5dcc8b1612c"
      },
      "source": [
        "doc1 = nlp(u'Free - Coupons for next movie. The above links will take you straight your partners site. For more information or to see other offers available, you can also visit  the Groupon on the Working Advantage w e b s i t e .')\n",
        "print(\"doc1=\", spam_vocab.similarity(doc1)) \n",
        "\n",
        "doc2 = nlp(u'Free- Coupons for next movie.  The   above links will take you  straight to our partners site. For more information or to see   other   offers   available, you can also visit the Groupon on the working Advantage  website .')\n",
        "print(\"doc2=\",spam_vocab.similarity(doc2)) \n",
        "\n",
        "doc3 = nlp(u'Our records indicate your Pension is under performing to see higher growth and up to 25% cash release reply PENSION for a free review. To opt out reply STOP')\n",
        "print(\"doc3=\",spam_vocab.similarity(doc3)) \n",
        "\n",
        "doc4 = nlp(u'Enter to win $25,000 and get a Free Hotel Night! Just click here for a $1 trial membership in NetMarket, the Internet spremier discount shopping site: Fast Company EZVenture gives you FREE business articles,PLUS, you could win YOUR CHOICE of a BMW Z3 convertible, $100,000, shares of Microsoft stock, or a home office computer. Go there and get your chances to win now. A crazy-funny-cool trivia book with a $10,000 prize? PLUS chocolate, nail polish, cats, barnyard animals, and more? ')\n",
        "print(\"doc4=\",spam_vocab.similarity(doc4)) \n",
        "\n",
        "doc5 = nlp(u'Dear recipient, Avangar Technologies announces the beginning of a new unprecendented global employment campaign.  Due to companys exploding growth Avangar is expanding business to the European region. During last employment campaign over 1500 people worldwide took part in Avangars business and more than half of them are currently employed by the company. And now we are offering you one more opportunity to earn extra money working with Avangar Technologies. We are looking for honest, responsible, hard-working people that can dedicate 2-4 hours of their time per day and earn extra  £300-500 weekly. All offered positions are currently part-time and give you a chance to work mainly from home.')\n",
        "print(\"doc5=\",spam_vocab.similarity(doc5)) \n",
        "\n",
        "doc6 = nlp(u'I know thats an incredible statement, but bear with me while I explain. You have already deleted mail from dozens of Get Rich Quick schemes, chain letter offers, and LOTS of other absurd scams that promise to make you rich overnight with no investment and no work. My offer isnot one of those. What I am offering is a  straightforward computer-based service that you can run full-or part-time like a regular business. This service runs auto-matically while you sleep, vacation, or work a regular job. It provides a valuable new service for businesses in your area. I am offering a high-tech, low-maintenance, work-from-anywhere business that can bring in a nice comfortable additional income for your family. I did it for eight years. Since I started inviting others to join me, I have helped over 4000 do the same.')\n",
        "print(\"doc6=\",spam_vocab.similarity(doc6)) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc1= 0.6377587778299161\n",
            "doc2= 0.6220590418671402\n",
            "doc3= 0.6411495501540143\n",
            "doc4= 0.672825838500943\n",
            "doc5= 0.6368650413866858\n",
            "doc6= 0.6158453398972168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb-9_n4Y52MC"
      },
      "source": [
        "# Method_3: Using TF-IDF Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC8raVlV7Wf5"
      },
      "source": [
        "spam_vocab = ('Free Click here  visit open attachment call this number money Out extra offer  available Pension Opportunity Chance Investment Pension')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G784sPf47l44"
      },
      "source": [
        "doc1 = ('Free - Coupons for next movie. The above links will take you straight your partners site. For more information or to see other offers available, you can also visit  the Groupon on the Working Advantage w e b s i t e .')\n",
        "\n",
        "\n",
        "doc2 = ('Free- Coupons for next movie.  The   above links will take you  straight to our partners site. For more information or to see   other   offers   available, you can also visit the Groupon on the working Advantage  website .')\n",
        "# print(\"doc2=\",spam_vocab.similarity(doc2)) \n",
        "\n",
        "doc3 = ('Our records indicate your Pension is under performing to see higher growth and up to 25% cash release reply PENSION for a free review. To opt out reply STOP')\n",
        "# print(\"doc3=\",spam_vocab.similarity(doc3)) \n",
        "\n",
        "doc4 = ('Enter to win $25,000 and get a Free Hotel Night! Just click here for a $1 trial membership in NetMarket, the Internet spremier discount shopping site: Fast Company EZVenture gives you FREE business articles,PLUS, you could win YOUR CHOICE of a BMW Z3 convertible, $100,000, shares of Microsoft stock, or a home office computer. Go there and get your chances to win now. A crazy-funny-cool trivia book with a $10,000 prize? PLUS chocolate, nail polish, cats, barnyard animals, and more? ')\n",
        "# print(\"doc4=\",spam_vocab.similarity(doc4)) \n",
        "\n",
        "doc5 = ('Dear recipient, Avangar Technologies announces the beginning of a new unprecendented global employment campaign.  Due to companys exploding growth Avangar is expanding business to the European region. During last employment campaign over 1500 people worldwide took part in Avangars business and more than half of them are currently employed by the company. And now we are offering you one more opportunity to earn extra money working with Avangar Technologies. We are looking for honest, responsible, hard-working people that can dedicate 2-4 hours of their time per day and earn extra  £300-500 weekly. All offered positions are currently part-time and give you a chance to work mainly from home.')\n",
        "# print(\"doc5=\",spam_vocab.similarity(doc5)) \n",
        "\n",
        "doc6 = ('I know thats an incredible statement, but bear with me while I explain. You have already deleted mail from dozens of Get Rich Quick schemes, chain letter offers, and LOTS of other absurd scams that promise to make you rich overnight with no investment and no work. My offer isnot one of those. What I am offering is a  straightforward computer-based service that you can run full-or part-time like a regular business. This service runs auto-matically while you sleep, vacation, or work a regular job. It provides a valuable new service for businesses in your area. I am offering a high-tech, low-maintenance, work-from-anywhere business that can bring in a nice comfortable additional income for your family. I did it for eight years. Since I started inviting others to join me, I have helped over 4000 do the same.')\n",
        "# print(\"doc6=\",spam_vocab.similarity(doc6)) "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjMXVPIh6ZcQ",
        "outputId": "1f30d0f0-1903-4813-f611-fcd74297e554"
      },
      "source": [
        "import nltk, string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt') # if necessary...\n",
        "\n",
        "\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
        "\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(item) for item in tokens]\n",
        "\n",
        "'''remove punctuation, lowercase, stem'''\n",
        "def normalize(text):\n",
        "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
        "\n",
        "def cosine_sim(text1, text2):\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    return ((tfidf * tfidf.T).A)[0,1]\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijPBjagg7zpz",
        "outputId": "46b88777-791c-437e-db3e-cf86be6aff15"
      },
      "source": [
        "print(\"doc1=\" , cosine_sim(doc1, spam_vocab))\n",
        "print(\"doc2=\" , cosine_sim(doc2, spam_vocab))\n",
        "print(\"doc3=\" , cosine_sim(doc3, spam_vocab))\n",
        "print(\"doc4=\" , cosine_sim(doc4, spam_vocab))\n",
        "print(\"doc5=\" , cosine_sim(doc5, spam_vocab))\n",
        "print(\"doc6=\" , cosine_sim(doc6, spam_vocab))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc1= 0.11031978563063616\n",
            "doc2= 0.13507348367087132\n",
            "doc3= 0.15339846575647692\n",
            "doc4= 0.06777726241770743\n",
            "doc5= 0.09246586732040242\n",
            "doc6= 0.07744957175637636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbf6cvC68Qpp"
      },
      "source": [
        "# References\n",
        "\n",
        "1. https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
        "2. https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html"
      ]
    }
  ]
}